{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a938c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store Utility Functions\n",
    "from typing import Any, List, Dict, Optional\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "\n",
    "class VectorStoreUtils:\n",
    "    @staticmethod\n",
    "    def batch_process_documents(\n",
    "        documents: List[Document],\n",
    "        batch_size: int = 100,\n",
    "        callback=None\n",
    "    ) -> List[List[Document]]:\n",
    "        \"\"\"Split documents into batches for efficient processing.\"\"\"\n",
    "        batches = [documents[i:i + batch_size] for i in range(0, len(documents), batch_size)]\n",
    "        if callback:\n",
    "            for i, batch in enumerate(batches):\n",
    "                callback(i, len(batches))\n",
    "        return batches\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_embeddings(embeddings: List[List[float]], dim: int = 3072) -> bool:\n",
    "        \"\"\"Validate embedding dimensions and values.\"\"\"\n",
    "        if not embeddings:\n",
    "            return False\n",
    "        return all(len(emb) == dim for emb in embeddings)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "        \"\"\"Calculate cosine similarity between vectors.\"\"\"\n",
    "        vec1 = np.array(vec1)\n",
    "        vec2 = np.array(vec2)\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def deduplicate_documents(\n",
    "        documents: List[Document],\n",
    "        threshold: float = 0.95,\n",
    "        embedding_fn: Any = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Remove near-duplicate documents based on embedding similarity.\"\"\"\n",
    "        if not embedding_fn:\n",
    "            return documents\n",
    "            \n",
    "        embeddings = [embedding_fn(doc.page_content) for doc in documents]\n",
    "        unique_docs = []\n",
    "        used_indices = set()\n",
    "        \n",
    "        for i, emb1 in enumerate(embeddings):\n",
    "            if i in used_indices:\n",
    "                continue\n",
    "                \n",
    "            unique_docs.append(documents[i])\n",
    "            used_indices.add(i)\n",
    "            \n",
    "            for j, emb2 in enumerate(embeddings[i+1:], i+1):\n",
    "                if VectorStoreUtils.calculate_similarity(emb1, emb2) > threshold:\n",
    "                    used_indices.add(j)\n",
    "                    \n",
    "        return unique_docs\n",
    "\n",
    "# Example usage:\n",
    "print(\"Vector Store Utilities loaded ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2011fe7",
   "metadata": {},
   "source": [
    "# Comprehensive Vector Store Comparison Guide\n",
    "\n",
    "## What are Vector Stores?\n",
    "Vector stores are specialized databases designed to store and efficiently search through vector embeddings (numerical representations of text, images, or other data). They are crucial for:\n",
    "- Semantic search applications\n",
    "- Recommendation systems\n",
    "- Large-scale document retrieval\n",
    "- RAG (Retrieval Augmented Generation) systems\n",
    "\n",
    "## Why Compare Different Vector Stores?\n",
    "Different vector stores have distinct characteristics that make them suitable for different use cases:\n",
    "\n",
    "1. **Pinecone**\n",
    "   - Cloud-based, fully managed\n",
    "   - Excellent scalability\n",
    "   - Production-ready\n",
    "   - Pay-as-you-go pricing\n",
    "\n",
    "2. **FAISS**\n",
    "   - Local, in-memory storage\n",
    "   - High performance\n",
    "   - Open source\n",
    "   - Great for smaller datasets\n",
    "\n",
    "3. **Milvus/Zilliz**\n",
    "   - Hybrid (self-hosted or cloud)\n",
    "   - High scalability\n",
    "   - Complex query support\n",
    "   - Good for large-scale deployments\n",
    "\n",
    "4. **Weaviate**\n",
    "   - Graph-based vector search\n",
    "   - Schema-based organization\n",
    "   - Multi-modal support\n",
    "   - Rich filtering capabilities\n",
    "\n",
    "## Learning Objectives\n",
    "Through this notebook, you'll learn:\n",
    "1. How to implement each vector store\n",
    "2. Pros and cons of each option\n",
    "3. Performance comparisons\n",
    "4. Best use cases for each store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb3114",
   "metadata": {},
   "source": [
    "# Vector Stores Comparison with LangChain\n",
    "\n",
    "This notebook demonstrates the implementation and usage of different vector stores:\n",
    "1. Pinecone (Cloud-based)\n",
    "2. FAISS (Local, in-memory)\n",
    "3. Milvus/Zilliz (Self-hosted/Cloud)\n",
    "4. Weaviate (Self-hosted/Cloud)\n",
    "\n",
    "We'll use the same dataset across all vector stores to compare their functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc223a",
   "metadata": {},
   "source": [
    "## Setting up Environment\n",
    "\n",
    "First, let's install all required packages and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54acb9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -q langchain-community langchain pinecone-client faiss-cpu pymilvus weaviate-client python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8b63cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Google API key found\n",
      "✅ Embeddings model initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1760016611.373995   78734 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1760016611.374578   78734 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Import common dependencies\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify Google API key\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"❌ GOOGLE_API_KEY not found in .env file\")\n",
    "print(\"✅ Google API key found\")\n",
    "\n",
    "# Initialize Gemini embeddings model\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"gemini-embedding-001\",  # or \"gemini-embedding-001\"\n",
    "    task_type=\"retrieval_document\",  # Specify task type for better embeddings\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "print(\"✅ Embeddings model initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75746d8",
   "metadata": {},
   "source": [
    "# Understanding Embeddings\n",
    "\n",
    "## What are Embeddings?\n",
    "Embeddings are dense vector representations of text or other data that capture semantic meaning. In this example, we use Google's Gemini model for creating embeddings.\n",
    "\n",
    "## Key Components:\n",
    "1. **Embedding Model**: \n",
    "   - Uses `GoogleGenerativeAIEmbeddings`\n",
    "   - Model: \"gemini-embedding-001\" (optimized for text)\n",
    "   - Task type: \"retrieval_document\" (specific to document search)\n",
    "\n",
    "## Why These Settings?\n",
    "- The Gemini model provides high-quality embeddings\n",
    "- Retrieval-specific embeddings are optimized for search\n",
    "- Environment variables keep API keys secure\n",
    "\n",
    "## Process Flow:\n",
    "1. Load API keys securely\n",
    "2. Initialize embedding model\n",
    "3. Convert text to vectors for storage\n",
    "4. Use these vectors for similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5395b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Python is a high-level programming language known for its simplicity and readability.\",\n",
    "        metadata={\"type\": \"programming\", \"language\": \"Python\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"JavaScript is a scripting language primarily used for web development.\",\n",
    "        metadata={\"type\": \"programming\", \"language\": \"JavaScript\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Machine Learning is a subset of AI that focuses on data and algorithms.\",\n",
    "        metadata={\"type\": \"technology\", \"field\": \"AI\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Deep Learning is part of machine learning based on artificial neural networks.\",\n",
    "        metadata={\"type\": \"technology\", \"field\": \"AI\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Docker is a platform for developing, shipping, and running applications in containers.\",\n",
    "        metadata={\"type\": \"technology\", \"field\": \"DevOps\"}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4645f4",
   "metadata": {},
   "source": [
    "# Sample Data Creation\n",
    "\n",
    "## Document Structure\n",
    "Each document in our test set contains:\n",
    "1. **Content**: Main text information\n",
    "2. **Metadata**: Additional context and categorization\n",
    "3. **Unique Identifiers**: For tracking and retrieval\n",
    "\n",
    "## Best Practices:\n",
    "1. **Diverse Data**: Include various topics and lengths\n",
    "2. **Realistic Content**: Use real-world-like examples\n",
    "3. **Rich Metadata**: Add useful filtering attributes\n",
    "4. **Consistent Format**: Maintain uniform structure\n",
    "\n",
    "## Testing Considerations:\n",
    "- Different document lengths\n",
    "- Various content types\n",
    "- Multiple metadata fields\n",
    "- Edge cases and special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585c09a",
   "metadata": {},
   "source": [
    "## 1. FAISS Vector Store\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search. It's local and in-memory, making it perfect for smaller datasets and quick experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b817b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store created successfully! ✅\n"
     ]
    }
   ],
   "source": [
    "# Initialize FAISS vector store with persistence\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "# Create a directory for storing the index\n",
    "persist_directory = \"faiss_index\"\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.makedirs(persist_directory)\n",
    "\n",
    "try:\n",
    "    # Try to load existing index\n",
    "    if os.path.exists(f\"{persist_directory}/index.faiss\"):\n",
    "        faiss_store = FAISS.load_local(persist_directory, embeddings)\n",
    "        print(\"Loaded existing FAISS index ✅\")\n",
    "    else:\n",
    "        # Create new index from documents\n",
    "        faiss_store = FAISS.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings\n",
    "        )\n",
    "        # Save the index\n",
    "        faiss_store.save_local(persist_directory)\n",
    "        print(\"Created and saved new FAISS index ✅\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with FAISS store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1861c4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is artificial intelligence?\n",
      "\n",
      "Results:\n",
      "\n",
      "Score: 0.2305298000574112\n",
      "Content: Machine Learning is a subset of AI that focuses on data and algorithms.\n",
      "Metadata: {'type': 'technology', 'field': 'AI'}\n",
      "\n",
      "Score: 0.2490173727273941\n",
      "Content: Deep Learning is part of machine learning based on artificial neural networks.\n",
      "Metadata: {'type': 'technology', 'field': 'AI'}\n"
     ]
    }
   ],
   "source": [
    "# Search with FAISS\n",
    "query = \"What is artificial intelligence?\"\n",
    "results = faiss_store.similarity_search_with_score(query, k=2)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nResults:\")\n",
    "for doc, score in results:\n",
    "    print(f\"\\nScore: {score}\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96957a38",
   "metadata": {},
   "source": [
    "# Pinecone section removed\n",
    "# To re-enable Pinecone later:\n",
    "# 1. Create an index named 'langchain-demo' in Pinecone console with dimension matching your embeddings (3072)\n",
    "# 2. Add PINECONE_API_KEY to your .env\n",
    "# 3. Re-insert the Pinecone cell or use LangChain's Pinecone.from_documents/from_existing_index\n",
    "# (Pinecone removed to avoid package/version conflicts during this demo)\n",
    "print(\"⚠️ Pinecone section is disabled in this notebook. Skipping Pinecone tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0bd024",
   "metadata": {},
   "source": [
    "### Finding Your Pinecone Environment\n",
    "\n",
    "To find your environment name:\n",
    "1. Log in to Pinecone Console (https://app.pinecone.io/)\n",
    "2. Go to \"API Keys\" in the left sidebar\n",
    "3. Look for \"Environment\" or \"Default Environment\"\n",
    "   - It will be something like `gcp-starter` or `us-east1-gcp-free`\n",
    "4. Copy this value and use it as your `PINECONE_ENV` in the .env file\n",
    "\n",
    "Example .env file:\n",
    "```\n",
    "PINECONE_API_KEY=a1b2c3d4-5e6f-7g8h-9i10-j11k12l13m14\n",
    "PINECONE_ENV=gcp-starter\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3551b52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to Pinecone!\n",
      "\n",
      "Active indexes: [{\n",
      "    \"name\": \"langchain-demo\",\n",
      "    \"metric\": \"cosine\",\n",
      "    \"host\": \"langchain-demo-7lvw08o.svc.aped-4627-b74a.pinecone.io\",\n",
      "    \"spec\": {\n",
      "        \"serverless\": {\n",
      "            \"cloud\": \"aws\",\n",
      "            \"region\": \"us-east-1\"\n",
      "        }\n",
      "    },\n",
      "    \"status\": {\n",
      "        \"ready\": true,\n",
      "        \"state\": \"Ready\"\n",
      "    },\n",
      "    \"vector_type\": \"dense\",\n",
      "    \"dimension\": 3072,\n",
      "    \"deletion_protection\": \"disabled\",\n",
      "    \"tags\": null\n",
      "}]\n",
      "\n",
      "Index Statistics:\n",
      "Dimension: 3072\n",
      "Metric: cosine\n",
      "Status: {'ready': True, 'state': 'Ready'}\n",
      "\n",
      "Index Statistics:\n",
      "Dimension: 3072\n",
      "Metric: cosine\n",
      "Status: {'ready': True, 'state': 'Ready'}\n"
     ]
    }
   ],
   "source": [
    "# Test Pinecone Connection\n",
    "import os\n",
    "from pinecone import Pinecone\n",
    "\n",
    "try:\n",
    "    api_key = os.getenv('PINECONE_API_KEY')\n",
    "    \n",
    "    if not api_key:\n",
    "        print(\"⚠️ PINECONE_API_KEY not found in .env file\")\n",
    "        print(\"\\nMake sure your .env file contains:\")\n",
    "        print(\"PINECONE_API_KEY=your_api_key_here\")\n",
    "    else:\n",
    "        # Initialize Pinecone with your specific configuration\n",
    "        pc = Pinecone(api_key=api_key)\n",
    "        \n",
    "        # List indexes\n",
    "        active_indexes = pc.list_indexes()\n",
    "        print(\"✅ Successfully connected to Pinecone!\")\n",
    "        print(f\"\\nActive indexes: {active_indexes}\")\n",
    "        \n",
    "        # Get index details\n",
    "        if \"langchain-demo\" in [index.name for index in active_indexes]:\n",
    "            index = pc.describe_index(\"langchain-demo\")\n",
    "            print(\"\\nIndex Statistics:\")\n",
    "            print(f\"Dimension: {index.dimension}\")\n",
    "            print(f\"Metric: {index.metric}\")\n",
    "            print(f\"Status: {index.status}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error connecting to Pinecone: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2dcd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize Pinecone with Google's embeddings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpinecone\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pinecone \u001b[38;5;28;01mas\u001b[39;00m LangchainPinecone\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Initialize Pinecone\u001b[39;00m\n",
      "File \u001b[0;32m/media/rgukt/data/RAG/venv/lib/python3.10/site-packages/pinecone/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m.. include:: ../README.md\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mException\u001b[0m: The official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK."
     ]
    }
   ],
   "source": [
    "# Initialize Pinecone with comprehensive error handling and setup\n",
    "import pinecone\n",
    "from langchain_community.vectorstores import Pinecone as LangchainPinecone\n",
    "import time\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "def setup_pinecone(\n",
    "    api_key: str,\n",
    "    environment: str,\n",
    "    index_name: str,\n",
    "    dimension: int = 3072,\n",
    "    metric: str = \"cosine\",\n",
    "    pod_type: str = \"p1\",\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Set up Pinecone with proper error handling and validation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Pinecone\n",
    "        pinecone.init(api_key=api_key, environment=environment)\n",
    "        print(\"✅ Pinecone initialized\")\n",
    "        \n",
    "        # Check if index exists\n",
    "        if index_name not in pinecone.list_indexes():\n",
    "            print(f\"Creating index '{index_name}'...\")\n",
    "            pinecone.create_index(\n",
    "                name=index_name,\n",
    "                dimension=dimension,\n",
    "                metric=metric,\n",
    "                pod_type=pod_type\n",
    "            )\n",
    "            # Wait for index to be ready\n",
    "            while not index_name in pinecone.list_indexes():\n",
    "                time.sleep(1)\n",
    "        \n",
    "        # Get index stats\n",
    "        index = pinecone.Index(index_name)\n",
    "        stats = index.describe_index_stats()\n",
    "        \n",
    "        return {\n",
    "            \"index\": index,\n",
    "            \"stats\": stats,\n",
    "            \"status\": \"ready\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error setting up Pinecone: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_pinecone_store(\n",
    "    documents: list,\n",
    "    embeddings: Any,\n",
    "    index_info: Dict[str, Any],\n",
    "    batch_size: int = 100\n",
    ") -> Optional[LangchainPinecone]:\n",
    "    \"\"\"\n",
    "    Create Pinecone vector store with batching and progress tracking.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create vector store\n",
    "        vector_store = LangchainPinecone.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings,\n",
    "            index_name=index_info[\"index\"]._index_name,\n",
    "            namespace=\"default\",\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating vector store: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Main execution with environment validation\n",
    "try:\n",
    "    # Get and validate environment variables\n",
    "    api_key = os.getenv('PINECONE_API_KEY')\n",
    "    env = os.getenv('PINECONE_ENV')\n",
    "    \n",
    "    if not api_key or not env:\n",
    "        raise ValueError(\n",
    "            \"Missing environment variables. Please set:\\n\"\n",
    "            \"- PINECONE_API_KEY\\n\"\n",
    "            \"- PINECONE_ENV\"\n",
    "        )\n",
    "    \n",
    "    # Setup Pinecone\n",
    "    index_info = setup_pinecone(\n",
    "        api_key=api_key,\n",
    "        environment=env,\n",
    "        index_name=\"langchain-demo\"\n",
    "    )\n",
    "    \n",
    "    if not index_info:\n",
    "        raise ValueError(\"Failed to setup Pinecone\")\n",
    "        \n",
    "    print(\"\\nPinecone Index Statistics:\")\n",
    "    print(f\"Total vectors: {index_info['stats'].total_vector_count}\")\n",
    "    print(f\"Namespaces: {index_info['stats'].namespaces}\")\n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store = create_pinecone_store(\n",
    "        documents=documents,\n",
    "        embeddings=embeddings,\n",
    "        index_info=index_info\n",
    "    )\n",
    "    \n",
    "    if vector_store:\n",
    "        print(\"\\n✅ Pinecone vector store created successfully!\")\n",
    "        \n",
    "        # Test search functionality\n",
    "        query = \"What is machine learning?\"\n",
    "        print(f\"\\nTest search query: {query}\")\n",
    "        \n",
    "        results = vector_store.similarity_search(\n",
    "            query=query,\n",
    "            k=2\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSearch Results:\")\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. Content: {doc.page_content[:100]}...\")\n",
    "            print(f\"   Metadata: {doc.metadata}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(\"\\n❌ Error in Pinecone setup:\")\n",
    "    print(str(e))\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Check your .env file has PINECONE_API_KEY and PINECONE_ENV\")\n",
    "    print(\"2. Verify your Pinecone account status\")\n",
    "    print(\"3. Check if you've reached your index limit\")\n",
    "    print(\"4. Verify network connectivity\")\n",
    "    print(\"\\nTo get your environment name:\")\n",
    "    print(\"1. Log in to Pinecone Console (https://app.pinecone.io/)\")\n",
    "    print(\"2. Go to API Keys section\")\n",
    "    print(\"3. Find your environment (e.g., 'gcp-starter')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ec02d",
   "metadata": {},
   "source": [
    "# Re-enabled Pinecone Integration with Best Practices\n",
    "\n",
    "## Why Pinecone?\n",
    "- Production-ready vector database\n",
    "- Automatic scaling\n",
    "- High availability\n",
    "- Real-time updates\n",
    "\n",
    "## Configuration Requirements:\n",
    "1. Pinecone Account Setup\n",
    "2. Environment Variables\n",
    "3. Index Configuration\n",
    "4. Error Handling\n",
    "\n",
    "This implementation includes:\n",
    "- Proper environment checks\n",
    "- Automated index creation\n",
    "- Connection retry logic\n",
    "- Error recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a82e86",
   "metadata": {},
   "source": [
    "### Testing Google Generative AI Embeddings\n",
    "\n",
    "Let's test the embeddings directly to understand how they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5fe032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Google's embedding model directly\n",
    "test_texts = [\n",
    "    \"Machine learning is amazing\",\n",
    "    \"Python programming is fun\",\n",
    "    \"AI technology is advancing rapidly\"\n",
    "]\n",
    "\n",
    "# Get embeddings for each text\n",
    "try:\n",
    "    # Single query embedding\n",
    "    single_embedding = embeddings.embed_query(test_texts[0])\n",
    "    print(f\"Single embedding dimension: {len(single_embedding)}\")\n",
    "    \n",
    "    # Multiple document embeddings\n",
    "    doc_embeddings = embeddings.embed_documents(test_texts)\n",
    "    print(f\"\\nNumber of document embeddings: {len(doc_embeddings)}\")\n",
    "    print(f\"Each document embedding dimension: {len(doc_embeddings[0])}\")\n",
    "    \n",
    "    print(\"\\n✅ Embeddings generated successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error generating embeddings: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b4010e",
   "metadata": {},
   "source": [
    "## 3. Milvus Vector Store\n",
    "\n",
    "Milvus is an open-source vector database that can be self-hosted or used via Zilliz Cloud. For this example, we'll use the Python client to connect to a local Milvus instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c6d3d",
   "metadata": {},
   "source": [
    "### Setting up Milvus with Docker\n",
    "\n",
    "To run Milvus locally, create a `docker-compose.yml` file with:\n",
    "```yaml\n",
    "version: '3.5'\n",
    "services:\n",
    "  etcd:\n",
    "    image: quay.io/coreos/etcd:v3.5.5\n",
    "    environment:\n",
    "      - ETCD_AUTO_COMPACTION_MODE=revision\n",
    "      - ETCD_AUTO_COMPACTION_RETENTION=1000\n",
    "      - ETCD_QUOTA_BACKEND_BYTES=4294967296\n",
    "    volumes:\n",
    "      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd\n",
    "    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd\n",
    "\n",
    "  milvus:\n",
    "    image: milvusdb/milvus:latest\n",
    "    environment:\n",
    "      - ETCD_ENDPOINTS=etcd:2379\n",
    "    volumes:\n",
    "      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus\n",
    "    ports:\n",
    "      - \"19530:19530\"\n",
    "    depends_on:\n",
    "      - etcd\n",
    "```\n",
    "\n",
    "Then run:\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a7de8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to create new connection using: ed2b3e4ea533416e908aff92ba76fb14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error initializing Milvus: <MilvusException: (code=2, message=Fail connecting to server on localhost:19530, illegal connection params or server unavailable)>\n",
      "\n",
      "Make sure you have Milvus running locally or update connection details for cloud deployment\n"
     ]
    }
   ],
   "source": [
    "# Initialize Milvus with proper error handling and connection management\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from pymilvus import connections, utility\n",
    "\n",
    "try:\n",
    "    # First, check if Milvus is running\n",
    "    connections.connect(\n",
    "        alias=\"default\",\n",
    "        host=\"localhost\",\n",
    "        port=\"19530\",\n",
    "        timeout=10  # 10 second timeout\n",
    "    )\n",
    "    \n",
    "    # Check server status\n",
    "    if utility.get_server_version():\n",
    "        print(\"Connected to Milvus server ✅\")\n",
    "        \n",
    "        try:\n",
    "            # Create vector store\n",
    "            milvus_store = Milvus.from_documents(\n",
    "                documents=documents,\n",
    "                embedding=embeddings,\n",
    "                connection_args={\"host\": \"localhost\", \"port\": \"19530\"},\n",
    "                collection_name=\"langchain_demo\",\n",
    "                drop_old=True  # Will recreate collection if it exists\n",
    "            )\n",
    "            print(\"Milvus vector store created successfully! ✅\")\n",
    "            \n",
    "            # Test search functionality\n",
    "            query = \"What is machine learning?\"\n",
    "            results = milvus_store.similarity_search(query, k=2)\n",
    "            print(\"\\nTest search results:\")\n",
    "            for i, doc in enumerate(results, 1):\n",
    "                print(f\"\\n{i}. {doc.page_content[:100]}...\")\n",
    "                \n",
    "        except Exception as store_error:\n",
    "            print(f\"Error creating vector store: {str(store_error)}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"❌ Could not verify Milvus server version\")\n",
    "        \n",
    "except Exception as conn_error:\n",
    "    print(\"❌ Could not connect to Milvus server\")\n",
    "    print(f\"Error: {str(conn_error)}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Make sure Milvus is running: docker ps | grep milvus\")\n",
    "    print(\"2. Check docker-compose logs: docker-compose logs milvus\")\n",
    "    print(\"3. Verify ports are open: netstat -an | grep 19530\")\n",
    "    \n",
    "finally:\n",
    "    # Clean up connection\n",
    "    try:\n",
    "        connections.disconnect(\"default\")\n",
    "        print(\"\\nMilvus connection closed properly\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6763549",
   "metadata": {},
   "source": [
    "## 4. Weaviate Vector Store\n",
    "\n",
    "Weaviate is an open-source vector search engine that can be self-hosted or used via Weaviate Cloud Services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1231fbf9",
   "metadata": {},
   "source": [
    "### Setting up Weaviate with Docker\n",
    "\n",
    "To run Weaviate locally, create a `docker-compose.weaviate.yml` file:\n",
    "```yaml\n",
    "version: '3.4'\n",
    "services:\n",
    "  weaviate:\n",
    "    image: semitechnologies/weaviate:latest\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    environment:\n",
    "      QUERY_DEFAULTS_LIMIT: 25\n",
    "      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n",
    "      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\n",
    "      DEFAULT_VECTORIZER_MODULE: 'none'\n",
    "      CLUSTER_HOSTNAME: 'node1'\n",
    "    volumes:\n",
    "      - weaviate_data:/var/lib/weaviate\n",
    "\n",
    "volumes:\n",
    "  weaviate_data:\n",
    "```\n",
    "\n",
    "Run with:\n",
    "```bash\n",
    "docker-compose -f docker-compose.weaviate.yml up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d71e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error initializing Weaviate: Client.__init__() got an unexpected keyword argument 'url'\n",
      "\n",
      "Make sure you have Weaviate running locally or update connection details for cloud deployment\n"
     ]
    }
   ],
   "source": [
    "# Initialize Weaviate with proper error handling and schema\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "import time\n",
    "\n",
    "try:\n",
    "    # Initialize client with retry logic\n",
    "    max_retries = 3\n",
    "    retry_delay = 2\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            client = weaviate.Client(\n",
    "                url=\"http://localhost:8080\",\n",
    "            )\n",
    "            # Test connection\n",
    "            client.schema.get()\n",
    "            print(\"Connected to Weaviate ✅\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Connection attempt {attempt + 1} failed, retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # Define class schema for our documents\n",
    "    class_name = \"RAGDocument\"\n",
    "    class_config = {\n",
    "        \"class\": class_name,\n",
    "        \"vectorizer\": \"none\",  # We'll provide our own vectors\n",
    "        \"properties\": [\n",
    "            {\n",
    "                \"name\": \"content\",\n",
    "                \"dataType\": [\"text\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"metadata\",\n",
    "                \"dataType\": [\"object\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create schema if it doesn't exist\n",
    "    if not client.schema.exists(class_name):\n",
    "        client.schema.create_class(class_config)\n",
    "        print(f\"Created schema for class {class_name} ✅\")\n",
    "    \n",
    "    # Create vector store\n",
    "    weaviate_store = Weaviate.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        client=client,\n",
    "        by_text=False,\n",
    "        index_name=class_name\n",
    "    )\n",
    "    print(\"Weaviate vector store created successfully! ✅\")\n",
    "    \n",
    "    # Test search\n",
    "    query = \"What is machine learning?\"\n",
    "    results = weaviate_store.similarity_search(query, k=2)\n",
    "    print(\"\\nTest search results:\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. {doc.page_content[:100]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error with Weaviate: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Check if Weaviate is running: docker ps | grep weaviate\")\n",
    "    print(\"2. Verify the API is accessible: curl http://localhost:8080/v1/.well-known/ready\")\n",
    "    print(\"3. Check Weaviate logs: docker-compose -f docker-compose.weaviate.yml logs weaviate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cdfd0b",
   "metadata": {},
   "source": [
    "## Vector Store Comparison\n",
    "\n",
    "Here's a quick comparison of the vector stores we've looked at:\n",
    "\n",
    "1. **FAISS**\n",
    "   - ✅ Local, in-memory storage\n",
    "   - ✅ Great for quick prototyping\n",
    "   - ✅ No external dependencies\n",
    "   - ❌ Not suitable for large-scale production\n",
    "\n",
    "2. **Pinecone**\n",
    "   - ✅ Fully managed cloud service\n",
    "   - ✅ Highly scalable\n",
    "   - ✅ Great for production\n",
    "   - ❌ Paid service\n",
    "\n",
    "3. **Milvus**\n",
    "   - ✅ Open source\n",
    "   - ✅ Can be self-hosted or cloud\n",
    "   - ✅ Highly scalable\n",
    "   - ❌ More complex setup\n",
    "\n",
    "4. **Weaviate**\n",
    "   - ✅ Advanced features (semantic search)\n",
    "   - ✅ GraphQL interface\n",
    "   - ✅ Can be self-hosted or cloud\n",
    "   - ❌ More resource intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0eb22",
   "metadata": {},
   "source": [
    "## Vector Store Performance Comparison\n",
    "\n",
    "Let's compare the performance of different vector stores across key metrics:\n",
    "1. Insertion Speed\n",
    "2. Query Speed\n",
    "3. Memory Usage\n",
    "4. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd909639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def measure_performance(\n",
    "    store_name: str,\n",
    "    vector_store: Any,\n",
    "    test_queries: List[str],\n",
    "    n_runs: int = 5\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Measure vector store performance metrics.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"name\": store_name,\n",
    "        \"query_times\": [],\n",
    "        \"memory_usage\": [],\n",
    "        \"results\": []\n",
    "    }\n",
    "    \n",
    "    process = psutil.Process()\n",
    "    \n",
    "    for query in test_queries:\n",
    "        query_times = []\n",
    "        for _ in range(n_runs):\n",
    "            # Clear memory\n",
    "            if hasattr(vector_store, 'clear_cache'):\n",
    "                vector_store.clear_cache()\n",
    "                \n",
    "            # Measure memory before\n",
    "            mem_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "            \n",
    "            # Time the query\n",
    "            start_time = time.time()\n",
    "            search_results = vector_store.similarity_search(query, k=2)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Measure memory after\n",
    "            mem_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "            \n",
    "            query_times.append(end_time - start_time)\n",
    "            results[\"memory_usage\"].append(mem_after - mem_before)\n",
    "            results[\"results\"].append(len(search_results))\n",
    "    \n",
    "        results[\"query_times\"].append(np.mean(query_times))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain Python programming\",\n",
    "    \"Tell me about DevOps\",\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Describe software development\"\n",
    "]\n",
    "\n",
    "# Run performance tests for each available store\n",
    "stores_to_test = {\n",
    "    \"FAISS\": faiss_store,\n",
    "    # \"Pinecone\": vector_store,  # Uncomment if Pinecone is configured\n",
    "    \"Milvus\": milvus_store if 'milvus_store' in locals() else None,\n",
    "    \"Weaviate\": weaviate_store if 'weaviate_store' in locals() else None\n",
    "}\n",
    "\n",
    "# Store results\n",
    "performance_results = {}\n",
    "\n",
    "for name, store in stores_to_test.items():\n",
    "    if store is not None:\n",
    "        try:\n",
    "            print(f\"\\nTesting {name}...\")\n",
    "            results = measure_performance(name, store, test_queries)\n",
    "            performance_results[name] = results\n",
    "            \n",
    "            print(f\"Average query time: {np.mean(results['query_times']):.4f} seconds\")\n",
    "            print(f\"Average memory usage: {np.mean(results['memory_usage']):.2f} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error testing {name}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ {name} not available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a1d04",
   "metadata": {},
   "source": [
    "### Notes — Pinecone removed for this demo\n",
    "\n",
    "- The Pinecone section has been disabled because the notebook experienced package/version conflicts during setup.\n",
    "- If you want to re-enable Pinecone later:\n",
    "  1. Create an index named `langchain-demo` in the Pinecone Console with dimension `3072` and metric `cosine`.\n",
    "  2. Add `PINECONE_API_KEY` to your `.env` file.\n",
    "  3. Re-insert the Pinecone cell or use LangChain's Pinecone helper methods.\n",
    "\n",
    "Troubleshooting:\n",
    "- Milvus: make sure Milvus is running locally on port `19530` or change `connection_args` to your Milvus host/port.\n",
    "- Weaviate: ensure `weaviate-client` is the expected version; the client initializer may accept `url=` or `base_url=` depending on version. Update `weaviate.Client(base_url=\"http://localhost:8080\")` if needed.\n",
    "\n",
    "If you want, I can re-enable Pinecone in the notebook with a pinned, working client version and exact code — or we can keep it disabled and proceed with FAISS/Milvus/Weaviate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
